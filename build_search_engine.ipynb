{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "semantic_search_engine.py\n",
    "\n",
    "A modular semantic search engine for a Reddit life-advice corpus.\n",
    "\n",
    "This version is configured for interactive use in notebooks.\n",
    "It loads a dataset from a hardcoded path and builds the FAISS index.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(iterable=None, *args, **kwargs):\n",
    "        return iterable if iterable is not None else range(0)\n",
    "\n",
    "\n",
    "class SemanticSearchEngine:\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 index_dir: str | Path = \"index\",\n",
    "                 use_gpu: bool = False):\n",
    "        self.index_dir = Path(index_dir)\n",
    "        self.index_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.index_path = self.index_dir / \"faiss.index\"\n",
    "        self.meta_path = self.index_dir / \"metadata.pkl\"\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        if use_gpu:\n",
    "            self.model = self.model.to(\"cuda\")\n",
    "\n",
    "        self.index: faiss.Index | None = None\n",
    "        self.metadata: List[Dict] = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalise(emb: np.ndarray) -> np.ndarray:\n",
    "        norm = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        return emb / np.maximum(norm, 1e-12)\n",
    "\n",
    "    def build(self, docs: List[str], metas: List[Dict], hnsw_m: int = 32):\n",
    "        if len(docs) != len(metas):\n",
    "            raise ValueError(\"docs and metas must have identical length\")\n",
    "\n",
    "        print(f\"Encoding {len(docs)} documents with {self.model_name} …\")\n",
    "        embeddings = self.model.encode(docs, batch_size=128, show_progress_bar=True, convert_to_numpy=True)\n",
    "        embeddings = self._normalise(embeddings.astype('float32'))\n",
    "\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexHNSWFlat(dim, hnsw_m, faiss.METRIC_INNER_PRODUCT)\n",
    "        self.index.hnsw.efConstruction = 200\n",
    "\n",
    "        print(\"Adding vectors to index …\")\n",
    "        self.index.add(embeddings)\n",
    "        self.metadata = metas\n",
    "        self.save()\n",
    "        print(f\"✓ Built index with {self.index.ntotal} vectors → {self.index_path.resolve()}\")\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"Index not initialised; call build() first\")\n",
    "        faiss.write_index(self.index, str(self.index_path))\n",
    "        with open(self.meta_path, \"wb\") as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "\n",
    "    def load(self):\n",
    "        if self.index is None:\n",
    "            print(\"Loading index into memory …\")\n",
    "            self.index = faiss.read_index(str(self.index_path))\n",
    "        if not self.metadata:\n",
    "            with open(self.meta_path, \"rb\") as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        self.load()\n",
    "        q_emb = self.model.encode([query], convert_to_numpy=True)\n",
    "        q_emb = self._normalise(q_emb.astype('float32'))\n",
    "\n",
    "        scores, idxs = self.index.search(q_emb, top_k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], idxs[0]):\n",
    "            item = self.metadata[idx].copy()\n",
    "            item[\"score\"] = float(score)\n",
    "            results.append(item)\n",
    "        return results\n",
    "\n",
    "    def rag_answer(self, query: str, context_k: int = 5, model: str = \"gpt-3.5-turbo-0125\",\n",
    "                   openai_api_key: str | None = None) -> str:\n",
    "        import textwrap\n",
    "        import openai\n",
    "\n",
    "        openai.api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not openai.api_key:\n",
    "            raise EnvironmentError(\"OPENAI_API_KEY missing; pass via env var or parameter\")\n",
    "\n",
    "        contexts = self.search(query, top_k=context_k)\n",
    "        concatenated = \"\\n\\n\".join(\n",
    "            [f\"Title: {c['title']}\\nURL: {c['url']}\" for c in contexts]\n",
    "        )\n",
    "\n",
    "        sys_prompt = \"You are a friendly assistant who provides concise, actionable life advice.\"\n",
    "        user_prompt = (\n",
    "            f\"Based on the Reddit advice below, answer the QUESTION in 3–5 bullet points.\\n\\n\"\n",
    "            f\"REDDIT ADVICE:\\n{concatenated}\\n\\nQUESTION: {query}\"\n",
    "        )\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": sys_prompt},\n",
    "                      {\"role\": \"user\", \"content\": user_prompt}],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return textwrap.fill(response.choices[0].message.content.strip(), 100)\n",
    "\n",
    "\n",
    "def parse_reddit_jsonl(path: str) -> tuple[List[str], List[Dict]]:\n",
    "    docs: List[str] = []\n",
    "    metas: List[Dict] = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in tqdm(fh, desc=\"Reading JSONL\"):\n",
    "            item = json.loads(line)\n",
    "            title = item.get(\"title\", \"\").strip()\n",
    "            body = item.get(\"body\", \"\").strip()\n",
    "            comments = \" \".join(item.get(\"top_comments\", []))\n",
    "            full_text = \" \".join([title, body, comments]).strip()\n",
    "\n",
    "            docs.append(full_text)\n",
    "            metas.append({\n",
    "                \"title\": title or full_text[:60] + \"…\",\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"post_score\": item.get(\"score\", 0),\n",
    "            })\n",
    "    return docs, metas\n",
    "\n",
    "\n",
    "# Automatically build the index when run as a script (for notebook/test use)\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"reddit_wisdom_data.jsonl\"  # <<<<<< SET YOUR PATH HERE\n",
    "\n",
    "    engine = SemanticSearchEngine(index_dir=\"index\")\n",
    "    docs, metas = parse_reddit_jsonl(dataset_path)\n",
    "    engine.build(docs, metas)\n",
    "    print(\"Search engine index built and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
