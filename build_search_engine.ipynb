{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219b4da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinkrawtzow/Library/CloudStorage/OneDrive-MichaelMoÃàhleundRainerBraker/Studium/Master/Semester2/Social Media Analytics/SMA_Capstone/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading local RAG model (flan-t5-large)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Reading JSONL: 14it [00:00, 8041.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Encoding 14 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Adding vectors to index...\n",
      "‚úÖ Index built with 14 vectors ‚Üí /Users/martinkrawtzow/Library/CloudStorage/OneDrive-MichaelMoÃàhleundRainerBraker/Studium/Master/Semester2/Social Media Analytics/SMA_Capstone/index/faiss.index\n",
      "üîç Search engine index built and saved.\n",
      "\n",
      "üí¨ Local RAG summary:\n",
      "Tip: Ask yourself what you want to achieve or why you want to exercise. Try to look at your goals as\n",
      "goals, instead of adversities.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "semantic_search_engine.py\n",
    "\n",
    "A local semantic search engine for Reddit life advice, with RAG summaries powered by Hugging Face.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x, *args, **kwargs): return x\n",
    "\n",
    "\n",
    "class SemanticSearchEngine:\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 index_dir: str | Path = \"index\",\n",
    "                 use_gpu: bool = False):\n",
    "        self.index_dir = Path(index_dir)\n",
    "        self.index_path = self.index_dir / \"faiss.index\"\n",
    "        self.meta_path = self.index_dir / \"metadata.pkl\"\n",
    "        self.index_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        if use_gpu:\n",
    "            self.model = self.model.to(\"cuda\")\n",
    "\n",
    "        self.index: faiss.Index | None = None\n",
    "        self.metadata: List[Dict] = []\n",
    "\n",
    "        # Local RAG model setup (Flan-T5)\n",
    "        print(\"üîç Loading local RAG model (flan-t5-large)...\")\n",
    "        self.rag_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.rag_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.rag_pipeline = pipeline(\"text2text-generation\", model=self.rag_model, tokenizer=self.rag_tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalise(emb: np.ndarray) -> np.ndarray:\n",
    "        norm = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        return emb / np.maximum(norm, 1e-12)\n",
    "\n",
    "    def build(self, docs: List[str], metas: List[Dict], hnsw_m: int = 32):\n",
    "        if len(docs) != len(metas):\n",
    "            raise ValueError(\"Mismatch between docs and metas\")\n",
    "\n",
    "        print(f\"üì¶ Encoding {len(docs)} documents...\")\n",
    "        embeddings = self.model.encode(docs, batch_size=128, show_progress_bar=True, convert_to_numpy=True)\n",
    "        embeddings = self._normalise(embeddings.astype('float32'))\n",
    "\n",
    "        dim = embeddings.shape[1]\n",
    "        self.index = faiss.IndexHNSWFlat(dim, hnsw_m, faiss.METRIC_INNER_PRODUCT)\n",
    "        self.index.hnsw.efConstruction = 200\n",
    "\n",
    "        print(\"üìå Adding vectors to index...\")\n",
    "        self.index.add(embeddings)\n",
    "        self.metadata = metas\n",
    "        self.save()\n",
    "        print(f\"‚úÖ Index built with {self.index.ntotal} vectors ‚Üí {self.index_path.resolve()}\")\n",
    "\n",
    "    def save(self):\n",
    "        faiss.write_index(self.index, str(self.index_path))\n",
    "        with open(self.meta_path, \"wb\") as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "\n",
    "    def load(self):\n",
    "        if self.index is None:\n",
    "            self.index = faiss.read_index(str(self.index_path))\n",
    "        if not self.metadata:\n",
    "            with open(self.meta_path, \"rb\") as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        self.load()\n",
    "        q_emb = self.model.encode([query], convert_to_numpy=True)\n",
    "        q_emb = self._normalise(q_emb.astype('float32'))\n",
    "\n",
    "        scores, idxs = self.index.search(q_emb, top_k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], idxs[0]):\n",
    "            item = self.metadata[idx].copy()\n",
    "            item[\"score\"] = float(score)\n",
    "            results.append(item)\n",
    "        return results\n",
    "\n",
    "    def local_rag_answer(self, query: str, context_k: int = 5) -> str:\n",
    "        contexts = self.search(query, top_k=context_k)\n",
    "        context_texts = [\n",
    "            f\"Title: {c['title']}\\nAdvice: {c.get('top_comment', '')}\"\n",
    "            for c in contexts if c.get(\"top_comment\")\n",
    "        ]\n",
    "\n",
    "        prompt_body = \"\\n\\n\".join(context_texts)\n",
    "        prompt = (\n",
    "            f\"Based on the Reddit advice below, answer the QUESTION in 3‚Äì5 bullet points.\\n\\n\"\n",
    "            f\"REDDIT ADVICE:\\n{prompt_body}\\n\\nQUESTION: {query}\"\n",
    "        )\n",
    "\n",
    "        # Truncate to model's limit (512 tokens)\n",
    "        inputs = self.rag_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "        result = self.rag_pipeline(prompt, max_length=256, do_sample=True, temperature=0.7)\n",
    "        return textwrap.fill(result[0][\"generated_text\"], 100)\n",
    "\n",
    "\n",
    "def parse_reddit_jsonl(path: str) -> tuple[List[str], List[Dict]]:\n",
    "    docs, metas = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in tqdm(fh, desc=\"Reading JSONL\"):\n",
    "            item = json.loads(line)\n",
    "            title = item.get(\"title\", \"\").strip()\n",
    "            selftext = item.get(\"selftext\", \"\").strip()\n",
    "            comments = item.get(\"comments\", [])\n",
    "            comment_str = \" \".join(comments).strip()\n",
    "            full_text = \" \".join([title, selftext, comment_str]).strip()\n",
    "\n",
    "            docs.append(full_text)\n",
    "            metas.append({\n",
    "                \"title\": title or full_text[:60] + \"‚Ä¶\",\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"post_score\": item.get(\"score\", 0),\n",
    "                \"subreddit\": item.get(\"subreddit\", \"\"),\n",
    "                \"created_utc\": item.get(\"created_utc\", \"\"),\n",
    "                \"top_comment\": comments[0] if comments else \"\",\n",
    "            })\n",
    "    return docs, metas\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"reddit_wisdom_data.jsonl\"\n",
    "\n",
    "    engine = SemanticSearchEngine(index_dir=\"index\")\n",
    "    docs, metas = parse_reddit_jsonl(dataset_path)\n",
    "    engine.build(docs, metas)\n",
    "    print(\"üîç Search engine index built and saved.\")\n",
    "\n",
    "    # Test\n",
    "    query = \"How do I stay motivated to exercise?\"\n",
    "    print(\"\\nüí¨ Local RAG summary:\")\n",
    "    print(engine.local_rag_answer(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
