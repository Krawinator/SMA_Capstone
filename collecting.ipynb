{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f496b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Fetching from r/LifeProTips\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) >= desired_total_posts:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Error on submission \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmission.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---- Load Environment Variables ----\n",
    "load_dotenv()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    ")\n",
    "\n",
    "# ---- Parameters ----\n",
    "subreddit_name = \"LifeProTips\"\n",
    "desired_total_posts = 10000\n",
    "top_n_comments = 3\n",
    "min_score_threshold = 100\n",
    "save_every = 500  # Save every N posts\n",
    "output_file = \"reddit_lpt_data.json\"\n",
    "\n",
    "results = []\n",
    "seen_ids = set()\n",
    "\n",
    "# ---- Try to Resume From Existing File ----\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "        seen_ids = {post[\"id\"] for post in results}\n",
    "    print(f\"‚úî Resumed with {len(results)} previously collected posts.\")\n",
    "\n",
    "# ---- Main Collection Loop ----\n",
    "print(f\"\\nüîç Fetching from r/{subreddit_name}\")\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "collected = 0\n",
    "\n",
    "for submission in subreddit.top(limit=None, time_filter=\"all\"):\n",
    "    if submission.id in seen_ids:\n",
    "        continue\n",
    "    if submission.stickied or submission.score < min_score_threshold:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        top_comments = [\n",
    "            comment.body.strip()\n",
    "            for comment in submission.comments[:top_n_comments]\n",
    "            if hasattr(comment, \"body\") and comment.body.strip()\n",
    "        ]\n",
    "\n",
    "        if not top_comments:\n",
    "            continue\n",
    "\n",
    "        post_data = {\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"title\": submission.title.strip(),\n",
    "            \"selftext\": submission.selftext.strip(),\n",
    "            \"score\": submission.score,\n",
    "            \"url\": submission.url,\n",
    "            \"id\": submission.id,\n",
    "            \"created_utc\": submission.created_utc,\n",
    "            \"comments\": top_comments\n",
    "        }\n",
    "\n",
    "        results.append(post_data)\n",
    "        seen_ids.add(submission.id)\n",
    "        collected += 1\n",
    "\n",
    "        if collected % 50 == 0:\n",
    "            print(f\"‚Üí {collected} posts collected...\")\n",
    "\n",
    "        if collected % save_every == 0:\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"üíæ Auto-saved after {collected} posts.\")\n",
    "\n",
    "        if len(results) >= desired_total_posts:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error on submission {submission.id}: {e}\")\n",
    "        time.sleep(2)\n",
    "\n",
    "print(f\"\\n‚úÖ Finished r/{subreddit_name}: Collected {collected} new posts\")\n",
    "\n",
    "# ---- Final Save ----\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nüéâ Total saved: {len(results)} posts ‚Üí {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c347a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 260 entries\n",
      "‚úì Processed 260 valid entries\n",
      "‚úó Skipped 0 empty or low-quality entries\n",
      "‚úì Saved cleaned data to /Users/martinkrawtzow/Library/CloudStorage/OneDrive-MichaelMoÃàhleundRainerBraker/Studium/Master/Semester2/Social Media Analytics/SMA_Capstone/reddit_wisdom_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "INPUT_PATH = Path(\"reddit_wisdom_data.json\")\n",
    "OUTPUT_PATH = Path(\"reddit_wisdom_data.jsonl\")\n",
    "\n",
    "# --- Bot/Spam Filter ---\n",
    "def is_valid_comment(text: str) -> bool:\n",
    "    lower = text.lower()\n",
    "    return not any([\n",
    "        \"i am a bot\" in lower,\n",
    "        \"this action was performed automatically\" in lower,\n",
    "        \"[removed]\" in lower,\n",
    "        \"[deleted]\" in lower,\n",
    "        \"moderator\" in lower,\n",
    "        \"http\" in lower and len(text) < 50  # Likely just a link\n",
    "    ])\n",
    "\n",
    "# --- Improved text cleaner ---\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --- Load raw data ---\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "processed = []\n",
    "skipped = 0\n",
    "\n",
    "for entry in raw_data:\n",
    "    title = entry.get(\"title\", \"\").strip()\n",
    "    selftext = entry.get(\"selftext\", \"\").strip()\n",
    "    comments = entry.get(\"comments\", [])\n",
    "\n",
    "    # Filter junk/bot comments\n",
    "    valid_comments = [c for c in comments if is_valid_comment(c)]\n",
    "\n",
    "    if not title and not selftext and not valid_comments:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    top_comment = valid_comments[0] if valid_comments else \"\"\n",
    "    combined_raw = f\"{title} {selftext} {' '.join(valid_comments)}\"\n",
    "    combined_clean = clean_text(combined_raw)\n",
    "\n",
    "    processed.append({\n",
    "        \"title\": title,\n",
    "        \"selftext\": selftext,\n",
    "        \"comments\": valid_comments,\n",
    "        \"top_comment\": top_comment,\n",
    "        \"combined_clean\": combined_clean,\n",
    "        \"url\": entry.get(\"url\", \"\"),\n",
    "        \"score\": entry.get(\"score\", 0),\n",
    "        \"subreddit\": entry.get(\"subreddit\", \"\"),\n",
    "        \"created_utc\": entry.get(\"created_utc\", None),\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Loaded {len(raw_data)} entries\")\n",
    "print(f\"‚úì Processed {len(processed)} valid entries\")\n",
    "print(f\"‚úó Skipped {skipped} empty or low-quality entries\")\n",
    "\n",
    "# --- Save cleaned JSONL ---\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for item in processed:\n",
    "        json.dump(item, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"‚úì Saved cleaned data to {OUTPUT_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385202d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
